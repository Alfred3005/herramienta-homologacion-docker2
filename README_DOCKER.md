# ğŸ³ DockerizaciÃ³n del Sistema de HomologaciÃ³n APF v5 con LLM Local

Sistema completamente dockerizado con **Phi-3.5 Mini** ejecutÃ¡ndose localmente vÃ­a **Ollama**. Optimizado para hardware con **6GB VRAM**.

---

## ğŸ“‹ Tabla de Contenidos

- [Requisitos](#-requisitos)
- [Arquitectura](#-arquitectura)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso](#-uso)
- [Monitoreo](#-monitoreo)
- [OptimizaciÃ³n](#-optimizaciÃ³n)
- [Troubleshooting](#-troubleshooting)
- [Comparativa](#-comparativa-local-vs-api)

---

## ğŸ”§ Requisitos

### Hardware MÃ­nimo

| Componente | MÃ­nimo | Recomendado |
|------------|--------|-------------|
| **RAM** | 16 GB | 24 GB |
| **VRAM** | 6 GB | 8-12 GB |
| **Disco** | 20 GB libres | 50 GB libres |
| **GPU** | NVIDIA con CUDA | NVIDIA RTX serie |
| **CPU** | 4 cores | 8+ cores |

### Software

- âœ… **Docker** >= 20.10
- âœ… **Docker Compose** >= 2.0
- âœ… **NVIDIA Docker Runtime** (nvidia-docker2)
- âœ… **Drivers NVIDIA** >= 525.60.13
- âœ… **CUDA** >= 12.0 (incluido en drivers)

### VerificaciÃ³n de Requisitos

```bash
# Verificar Docker
docker --version
docker compose version

# Verificar GPU NVIDIA
nvidia-smi

# Verificar NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

---

## ğŸ—ï¸ Arquitectura

### Diagrama de Contenedores

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Docker Host (Ubuntu/Linux)         â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   apf-homologacion   â”‚  â”‚   apf-ollama    â”‚ â”‚
â”‚  â”‚   (Streamlit App)    â”‚â—„â”€â”¤   (LLM Server)  â”‚ â”‚
â”‚  â”‚                      â”‚  â”‚                 â”‚ â”‚
â”‚  â”‚   Port: 8501         â”‚  â”‚   Port: 11434   â”‚ â”‚
â”‚  â”‚   CPU only           â”‚  â”‚   GPU enabled   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â†“                        â†“            â”‚
â”‚      [app_data]             [ollama_models]     â”‚
â”‚      [app_cache]                 (~3GB)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                           â†“
   Usuario Web                  GPU (6GB VRAM)
  (localhost:8501)
```

### Componentes

1. **apf-homologacion** (Contenedor Python + Streamlit)
   - Interfaz web del sistema
   - LÃ³gica de validaciÃ³n
   - ComunicaciÃ³n con Ollama

2. **apf-ollama** (Contenedor Ollama)
   - Servidor de LLM local
   - Phi-3.5 Mini pre-cargado
   - Optimizado para 6GB VRAM

3. **apf-ollama-init** (Contenedor temporal)
   - Descarga Phi-3.5 Mini al inicio
   - Se ejecuta una vez y termina

---

## ğŸ“¦ InstalaciÃ³n

### Paso 1: Instalar Docker + NVIDIA Runtime

#### Ubuntu/Debian

```bash
# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Agregar usuario al grupo docker
sudo usermod -aG docker $USER
newgrp docker

# Instalar NVIDIA Docker
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# Verificar instalaciÃ³n
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

### Paso 2: Clonar Repositorio

```bash
cd ~
git clone https://github.com/Alfred3005/herramienta-homologacion-v5.git
cd herramienta-homologacion-v5
```

### Paso 3: Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.docker .env

# Editar si es necesario (opcional)
nano .env
```

**ConfiguraciÃ³n por defecto (.env):**
```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://ollama:11434
LLM_MODEL=phi3.5
MAX_CONCURRENT_ANALYSIS=1
OLLAMA_MAX_LOADED_MODELS=1
```

### Paso 4: Construir e Iniciar Contenedores

```bash
# Construir imÃ¡genes
docker compose build

# Iniciar servicios (incluye descarga automÃ¡tica de Phi-3.5)
docker compose up -d

# Ver logs en tiempo real
docker compose logs -f
```

**â³ Primera vez:** La descarga de Phi-3.5 Mini toma **5-15 minutos** (~2.3GB).

### Paso 5: Verificar InstalaciÃ³n

```bash
# Verificar que todos los contenedores estÃ©n corriendo
docker compose ps

# Verificar logs de Ollama
docker compose logs ollama

# Verificar logs de la app
docker compose logs app

# Probar conexiÃ³n con Ollama
curl http://localhost:11434/api/tags
```

**Salida esperada:**
```json
{
  "models": [
    {
      "name": "phi3.5",
      "size": 2300000000,
      ...
    }
  ]
}
```

---

## ğŸš€ Uso

### Acceder a la AplicaciÃ³n

1. Abrir navegador: **http://localhost:8501**
2. La interfaz de Streamlit deberÃ­a cargar
3. Subir archivo Excel con puestos
4. Subir PDF del reglamento
5. Ejecutar anÃ¡lisis

### Comandos BÃ¡sicos

```bash
# Iniciar servicios
docker compose up -d

# Detener servicios
docker compose down

# Ver logs
docker compose logs -f

# Reiniciar solo la app (sin reiniciar Ollama)
docker compose restart app

# Reiniciar todo
docker compose restart

# Ver uso de recursos
docker stats

# Acceder al contenedor de la app
docker compose exec app bash

# Acceder al contenedor de Ollama
docker compose exec ollama bash
```

### Monitorear VRAM

```bash
# Una vez
./docker/monitor-vram.sh

# Monitoreo continuo
./docker/monitor-vram.sh --continuous
```

**Salida esperada:**
```
ğŸ“Š Uso de VRAM:
   Total:       6144 MB (6.00 GB)
   Usada:       4800 MB (4.69 GB)
   Libre:       1344 MB (1.31 GB)
   Porcentaje:  78.1%

ğŸŒ¡ï¸  Temperatura: 68Â°C
âš™ï¸  UtilizaciÃ³n: 85%
```

---

## ğŸ” Monitoreo

### Ver Logs en Tiempo Real

```bash
# Todos los servicios
docker compose logs -f

# Solo Ollama
docker compose logs -f ollama

# Solo App
docker compose logs -f app

# Ãšltimas 100 lÃ­neas
docker compose logs --tail=100
```

### Verificar Salud de Servicios

```bash
# Health checks
docker compose ps

# Inspeccionar contenedor
docker inspect apf-ollama
docker inspect apf-homologacion
```

### MÃ©tricas de Rendimiento

```bash
# Uso de CPU, RAM, VRAM en tiempo real
docker stats

# Solo contenedores APF
docker stats apf-homologacion apf-ollama
```

### Dashboard Web (Opcional)

Instalar Portainer para UI web:

```bash
docker run -d -p 9000:9000 --name portainer \
  --restart=always \
  -v /var/run/docker.sock:/var/run/docker.sock \
  portainer/portainer-ce:latest
```

Acceder: **http://localhost:9000**

---

## âš¡ OptimizaciÃ³n

### Reducir Uso de VRAM

Si experimentas errores de OOM (Out of Memory):

**1. Usar cuantizaciÃ³n mÃ¡s agresiva:**

```bash
# Editar docker-compose.yml, cambiar modelo a versiÃ³n Q4
# En lugar de "phi3.5", usar "phi3.5:q4_0"
```

**2. Reducir context window:**

Editar `.env`:
```bash
OLLAMA_CONTEXT_SIZE=2048  # Default: 4096
```

**3. Desactivar modelos adicionales:**

```bash
# Asegurar solo 1 modelo cargado
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_NUM_PARALLEL=1
```

### Mejorar Velocidad

**1. Usar SSD para volÃºmenes:**

```bash
# Mover volÃºmenes Docker a SSD
sudo systemctl stop docker
sudo mv /var/lib/docker /mnt/ssd/docker
sudo ln -s /mnt/ssd/docker /var/lib/docker
sudo systemctl start docker
```

**2. Pre-calentar el modelo:**

```bash
# Ejecutar anÃ¡lisis dummy al inicio
docker compose exec ollama ollama run phi3.5 "test"
```

**3. Ajustar parÃ¡metros de Ollama:**

Editar `.env`:
```bash
OLLAMA_FLASH_ATTENTION=1  # Reduce latencia
OLLAMA_NUM_GPU=1           # Usar solo 1 GPU
```

### Reducir Espacio en Disco

```bash
# Limpiar imÃ¡genes no usadas
docker system prune -a

# Limpiar volÃºmenes huÃ©rfanos
docker volume prune

# Verificar espacio usado
docker system df
```

---

## ğŸ› ï¸ Troubleshooting

### Problema 1: Ollama no inicia

**SÃ­ntomas:**
```
Error: CUDA out of memory
Error: Could not load model
```

**SoluciÃ³n:**
```bash
# Verificar VRAM disponible
nvidia-smi

# Cerrar otros procesos usando GPU
pkill -f python
pkill -f ollama

# Reiniciar contenedor con lÃ­mites
docker compose down
docker compose up -d
```

### Problema 2: Modelo no descarga

**SÃ­ntomas:**
```
Error: Model not found
Error: Connection refused to ollama:11434
```

**SoluciÃ³n:**
```bash
# Descargar manualmente
docker compose exec ollama ollama pull phi3.5

# Verificar descarga
docker compose exec ollama ollama list

# Ver logs de descarga
docker compose logs ollama-init
```

### Problema 3: App no conecta con Ollama

**SÃ­ntomas:**
```
LLMProviderError: Could not connect to Ollama
Connection refused: http://ollama:11434
```

**SoluciÃ³n:**
```bash
# Verificar que Ollama estÃ© corriendo
docker compose ps ollama

# Probar conexiÃ³n manual
docker compose exec app curl http://ollama:11434/api/tags

# Verificar red Docker
docker network inspect herramienta-homologacion-v5_apf-network

# Reiniciar servicios
docker compose restart
```

### Problema 4: Respuestas lentas o timeout

**SÃ­ntomas:**
```
LLMProviderTimeoutError: Timeout after 120s
```

**SoluciÃ³n:**

Editar `.env`:
```bash
# Aumentar timeout
LLM_TIMEOUT=300  # 5 minutos

# Reducir tokens generados
MAX_TOKENS=1000  # Reducir si es muy largo
```

### Problema 5: VRAM insuficiente

**SÃ­ntomas:**
```
CUDA out of memory
RuntimeError: GPU memory exhausted
```

**SoluciÃ³n:**
```bash
# OpciÃ³n 1: Usar modelo mÃ¡s pequeÃ±o
docker compose down
# Editar .env: LLM_MODEL=llama3.2:1b
docker compose up -d

# OpciÃ³n 2: Usar cuantizaciÃ³n agresiva
# Editar .env: LLM_MODEL=phi3.5:q4_0

# OpciÃ³n 3: Liberar VRAM
docker compose restart ollama
```

### Problema 6: Puerto 8501 ya en uso

**SÃ­ntomas:**
```
Error: bind: address already in use
```

**SoluciÃ³n:**
```bash
# Encontrar proceso usando el puerto
sudo lsof -i :8501

# Matar proceso
kill -9 <PID>

# O cambiar puerto en docker-compose.yml
# ports: - "8502:8501"
```

---

## ğŸ“Š Comparativa: Local vs API

### PrecisiÃ³n

| Criterio | GPT-4o-mini (API) | Phi-3.5 Mini (Local) | Diferencia |
|----------|-------------------|----------------------|------------|
| Criterio 1 | 85% | ~72% | -13% |
| Criterio 2 | 87% | ~75% | -12% |
| Criterio 3 | 86% | ~73% | -13% |
| **Promedio** | **86%** | **73%** | **-13%** |

### Velocidad

| Volumen | GPT-4o-mini (API) | Phi-3.5 Mini (Local) | Diferencia |
|---------|-------------------|----------------------|------------|
| 1 puesto | ~60s | ~180s | 3x mÃ¡s lento |
| 10 puestos | ~10 min | ~30 min | 3x mÃ¡s lento |
| 25 puestos | ~15 min | ~60 min | 4x mÃ¡s lento |

### Costos

| Volumen | GPT-4o-mini (API) | Phi-3.5 Mini (Local) | Ahorro |
|---------|-------------------|----------------------|--------|
| 100 puestos | $35 MXN | $0 MXN | 100% |
| 1,000 puestos | $350 MXN | $0 MXN | 100% |
| Electricidad | N/A | ~$5 MXN/mes | Despreciable |

### Trade-offs

| Aspecto | API (GPT-4o-mini) | Local (Phi-3.5) |
|---------|-------------------|-----------------|
| **PrecisiÃ³n** | â­â­â­â­â­ Alta (86%) | â­â­â­ Media (73%) |
| **Velocidad** | â­â­â­â­â­ RÃ¡pida | â­â­ Lenta (3-4x) |
| **Costo** | â­â­â­ $0.35/puesto | â­â­â­â­â­ $0 |
| **Privacidad** | â­â­ Datos en cloud | â­â­â­â­â­ 100% local |
| **Setup** | â­â­â­â­â­ FÃ¡cil | â­â­â­ Medio |
| **Mantenimiento** | â­â­â­â­â­ Ninguno | â­â­â­ Moderado |
| **Escalabilidad** | â­â­â­â­â­ Ilimitada | â­â­ Limitada por HW |

**RecomendaciÃ³n:**
- **API:** Si necesitas mÃ¡xima precisiÃ³n y velocidad, presupuesto bajo ($35/mes para 100 puestos)
- **Local:** Si necesitas privacidad total, costo $0, y puedes aceptar -13% precisiÃ³n + 3x lentitud

---

## ğŸ¯ Casos de Uso Ideales para Docker Local

### âœ… Recomendado

- Pruebas de concepto (POC)
- VolÃºmenes bajos (<100 puestos/mes)
- Datos altamente sensibles (gobierno/militar)
- Sin presupuesto para APIs
- Desarrollo y experimentaciÃ³n
- Ambientes sin internet

### âŒ No Recomendado

- ProducciÃ³n con altos volÃºmenes (>500 puestos/mes)
- Necesidad de mÃ¡xima precisiÃ³n (>85%)
- Tiempo de respuesta crÃ­tico (<1 min/puesto)
- Hardware limitado (<6GB VRAM)
- Equipo sin conocimientos Docker/Linux

---

## ğŸ”„ ActualizaciÃ³n del Sistema

### Actualizar CÃ³digo de la App

```bash
# Pull Ãºltimos cambios
git pull origin main

# Reconstruir solo la app
docker compose build app

# Reiniciar app (Ollama sigue corriendo)
docker compose up -d app
```

### Actualizar Modelo de Ollama

```bash
# Descargar nueva versiÃ³n
docker compose exec ollama ollama pull phi3.5:latest

# Listar modelos
docker compose exec ollama ollama list

# Eliminar modelo antiguo (opcional)
docker compose exec ollama ollama rm phi3.5:old_version
```

### Actualizar Ollama

```bash
# Pull nueva imagen
docker pull ollama/ollama:latest

# Recrear contenedor
docker compose up -d --force-recreate ollama
```

---

## ğŸ—‘ï¸ DesinstalaciÃ³n

### Eliminar Contenedores y VolÃºmenes

```bash
# Detener y eliminar todo
docker compose down -v

# Eliminar imÃ¡genes
docker rmi ollama/ollama:latest
docker rmi herramienta-homologacion-v5-app

# Limpiar sistema completo
docker system prune -a --volumes
```

### Eliminar ConfiguraciÃ³n

```bash
# Eliminar archivos de configuraciÃ³n
rm -rf ~/.ollama
rm -f .env

# Eliminar repositorio
cd ..
rm -rf herramienta-homologacion-v5
```

---

## ğŸ“š Recursos Adicionales

- **DocumentaciÃ³n Ollama:** https://ollama.ai/docs
- **Modelos disponibles:** https://ollama.ai/library
- **Phi-3.5 Mini:** https://ollama.ai/library/phi3.5
- **Docker Compose:** https://docs.docker.com/compose/
- **NVIDIA Docker:** https://github.com/NVIDIA/nvidia-docker

---

## ğŸ†˜ Soporte

**Problemas tÃ©cnicos:**
- GitHub Issues: https://github.com/Alfred3005/herramienta-homologacion-v5/issues

**DocumentaciÃ³n del proyecto:**
- AnÃ¡lisis de viabilidad: [ANALISIS_DOCKERIZACION_LLM_LOCAL.md](./ANALISIS_DOCKERIZACION_LLM_LOCAL.md)
- Reporte ejecutivo: [REPORTE_EJECUTIVO_VERSION_5.md](./REPORTE_EJECUTIVO_VERSION_5.md)

---

**VersiÃ³n:** 1.0
**Fecha:** Noviembre 2025
**Mantenido por:** Sistema de HomologaciÃ³n APF v5
