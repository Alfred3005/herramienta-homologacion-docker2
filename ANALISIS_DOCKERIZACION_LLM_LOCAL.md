# AnÃ¡lisis de Viabilidad: DockerizaciÃ³n con LLMs Locales

**Fecha:** 23 de Noviembre, 2025
**Hardware Target:** 16GB RAM + 6GB VRAM
**Objetivo:** Ejecutar Sistema de HomologaciÃ³n APF v5 con LLMs locales en Docker

---

## 1. ANÃLISIS DEL SISTEMA ACTUAL

### 1.1 Requerimientos de LLM

**Uso actual con GPT-4o-mini:**
- **26 llamadas LLM** por puesto (12 funciones promedio)
- **~66,000 tokens** por puesto (49K input + 17K output)
- **Tareas que realiza el LLM:**
  1. **AdvancedQualityValidator:** Detectar duplicados, funciones malformadas, problemas legales
  2. **Criterio 1 (AnÃ¡lisis SemÃ¡ntico):** Evaluar fortaleza de verbos en contexto
  3. **Criterio 2 (ValidaciÃ³n Contextual):** Verificar respaldo normativo institucional
  4. **Criterio 3 (Impacto JerÃ¡rquico):** Validar coherencia con nivel del puesto

**Capacidades requeridas del LLM:**
- âœ… Razonamiento complejo sobre normativas legales
- âœ… AnÃ¡lisis semÃ¡ntico de funciones administrativas
- âœ… ComparaciÃ³n contextual con documentos largos (reglamentos)
- âœ… GeneraciÃ³n de justificaciones detalladas
- âœ… Salida en formato JSON estructurado
- âœ… ComprensiÃ³n de jerarquÃ­as organizacionales

### 1.2 Componentes Adicionales

**Embeddings (sentence-transformers):**
- Actualmente usa modelos de embeddings para bÃºsqueda semÃ¡ntica
- Modelos tÃ­picos: `all-MiniLM-L6-v2` (~80MB) o `paraphrase-multilingual-mpnet-base-v2` (~420MB)
- **Uso de VRAM:** ~500MB-1GB (menor prioridad)

**Memoria estimada actual:**
- Python + Streamlit: ~500MB
- Sentence-transformers: ~1GB
- LLM local: **Â¿? GB** (el cuello de botella)
- PDFs + datos en memoria: ~500MB
- **Total sin LLM:** ~2GB RAM

---

## 2. RESTRICCIONES DE HARDWARE

### 2.1 AnÃ¡lisis de 6GB VRAM

**Â¿QuÃ© cabe en 6GB VRAM?**

| Modelo | TamaÃ±o | VRAM MÃ­nima | VRAM Recomendada | Â¿Cabe? |
|--------|--------|-------------|------------------|--------|
| **Llama 3.2 1B** | 1B params | ~2GB | 3GB | âœ… SÃ |
| **Llama 3.2 3B** | 3B params | ~4GB | 5GB | âœ… SÃ (ajustado) |
| **Phi-3 Mini (3.8B)** | 3.8B params | ~4.5GB | 6GB | âœ… SÃ (lÃ­mite) |
| **Phi-3.5 Mini (3.8B)** | 3.8B params | ~4.5GB | 6GB | âœ… SÃ (lÃ­mite) |
| **Qwen 2.5 3B** | 3B params | ~4GB | 5GB | âœ… SÃ (ajustado) |
| **Llama 3.1 8B** | 8B params | ~8GB | 10GB | âŒ NO |
| **Mistral 7B** | 7B params | ~7GB | 9GB | âŒ NO |
| **Gemma 2 9B** | 9B params | ~10GB | 12GB | âŒ NO |

**ConclusiÃ³n:** Solo modelos de **1B-4B parÃ¡metros** son viables.

### 2.2 Trade-offs de Modelos PequeÃ±os (1B-4B)

**Ventajas:**
- âœ… Caben en 6GB VRAM
- âœ… Inferencia rÃ¡pida (importante con 26 llamadas/puesto)
- âœ… Bajo consumo de energÃ­a

**Desventajas:**
- âŒ Menor capacidad de razonamiento complejo
- âŒ Menos precisiÃ³n en anÃ¡lisis legal/normativo
- âŒ Mayor dificultad para seguir instrucciones complejas
- âŒ Posible degradaciÃ³n en calidad de validaciones

---

## 3. OPCIONES DE LLMS LOCALES (6GB VRAM)

### OpciÃ³n A: Phi-3.5 Mini (3.8B) - **RECOMENDADO**

**Especificaciones:**
- **TamaÃ±o:** 3.8B parÃ¡metros
- **VRAM:** ~4.5-5GB (con cuantizaciÃ³n Q4)
- **Context window:** 128K tokens
- **Proveedor:** Microsoft
- **Licencia:** MIT (uso comercial permitido)

**Ventajas:**
- ğŸ† Mejor relaciÃ³n calidad/tamaÃ±o en modelos pequeÃ±os
- âœ… Entrenado especÃ­ficamente para razonamiento y seguimiento de instrucciones
- âœ… Soporta JSON mode nativo
- âœ… 128K de contexto (ideal para reglamentos largos)
- âœ… MultilingÃ¼e (incluye espaÃ±ol)
- âœ… Rendimiento cercano a modelos 7B en benchmarks

**Desventajas:**
- âš ï¸ Inferior a GPT-4o-mini en razonamiento complejo
- âš ï¸ Puede requerir prompts mÃ¡s detallados
- âš ï¸ EspaÃ±ol no es su fortaleza principal

**Benchmarks (comparado con GPT-4o-mini):**
- Razonamiento: ~70-75% del rendimiento
- Seguimiento de instrucciones: ~80%
- JSON structuring: ~85%

**EstimaciÃ³n de precisiÃ³n del sistema:**
- Criterio 1: 85% â†’ ~70-75%
- Criterio 2: 87% â†’ ~72-77%
- Criterio 3: 86% â†’ ~71-76%
- **Promedio:** 86% â†’ ~71-76% (pÃ©rdida de 10-15%)

### OpciÃ³n B: Qwen 2.5 3B - **ALTERNATIVA FUERTE**

**Especificaciones:**
- **TamaÃ±o:** 3B parÃ¡metros
- **VRAM:** ~4GB (con cuantizaciÃ³n Q4)
- **Context window:** 32K tokens
- **Proveedor:** Alibaba Cloud
- **Licencia:** Apache 2.0

**Ventajas:**
- âœ… Excelente en seguimiento de instrucciones
- âœ… Fuerte en razonamiento matemÃ¡tico/lÃ³gico
- âœ… Menor uso de VRAM que Phi-3.5
- âœ… RÃ¡pido en inferencia

**Desventajas:**
- âš ï¸ Solo 32K de contexto (puede ser limitante para reglamentos largos)
- âš ï¸ EspaÃ±ol menos refinado
- âš ï¸ Menor rendimiento en tareas legales/normativas

### OpciÃ³n C: Llama 3.2 3B - **MÃS CONSERVADORA**

**Especificaciones:**
- **TamaÃ±o:** 3B parÃ¡metros
- **VRAM:** ~4GB (con cuantizaciÃ³n Q4)
- **Context window:** 128K tokens
- **Proveedor:** Meta
- **Licencia:** Llama 3.2 Community License

**Ventajas:**
- âœ… Familia Llama (ampliamente probada)
- âœ… 128K de contexto
- âœ… Buen soporte multilingÃ¼e
- âœ… Comunidad grande y activa

**Desventajas:**
- âš ï¸ Rendimiento inferior a Phi-3.5 y Qwen 2.5 en benchmarks
- âš ï¸ No optimizado especÃ­ficamente para razonamiento
- âš ï¸ Puede requerir mÃ¡s fine-tuning

### OpciÃ³n D: Llama 3.2 1B - **MÃXIMA VELOCIDAD, MENOR CALIDAD**

**Especificaciones:**
- **TamaÃ±o:** 1B parÃ¡metros
- **VRAM:** ~2GB
- **Context window:** 128K tokens

**Ventajas:**
- âœ… Muy rÃ¡pido (importante con 26 llamadas/puesto)
- âœ… Deja margen para otros componentes
- âœ… Bajo consumo de recursos

**Desventajas:**
- âŒ Calidad significativamente inferior
- âŒ No recomendado para razonamiento complejo
- âŒ EstimaciÃ³n: ~50-60% de precisiÃ³n vs sistema actual

**Veredicto:** âŒ NO RECOMENDADO para este caso de uso

---

## 4. ARQUITECTURAS DE IMPLEMENTACIÃ“N

### Arquitectura 1: Docker Single-Container (MonolÃ­tico)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Docker Container (Ubuntu)       â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Streamlit App (Frontend)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Sistema ValidaciÃ³n v5      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Ollama + Phi-3.5 Mini      â”‚  â”‚
â”‚  â”‚   (LLM Server Local)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   GPU (6GB VRAM)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Expuesto: Puerto 8501 (Streamlit)
```

**Ventajas:**
- âœ… ConfiguraciÃ³n simple
- âœ… Todo en un solo contenedor
- âœ… FÃ¡cil de distribuir

**Desventajas:**
- âš ï¸ DifÃ­cil de escalar
- âš ï¸ Si falla algo, todo falla
- âš ï¸ Uso menos eficiente de recursos

### Arquitectura 2: Docker Multi-Container (Microservicios) - **RECOMENDADO**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Container 1:       â”‚      â”‚  Container 2:        â”‚
â”‚  Streamlit + App    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Ollama + LLM        â”‚
â”‚  (CPU only)         â”‚ HTTP â”‚  (GPU enabled)       â”‚
â”‚  Puerto: 8501       â”‚      â”‚  Puerto: 11434       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                              â†“
    Host RAM (8GB)              Host VRAM (6GB)

Orquestado por: docker-compose.yml
```

**Ventajas:**
- âœ… SeparaciÃ³n de responsabilidades
- âœ… LLM se puede escalar independientemente
- âœ… MÃ¡s fÃ¡cil debuggear
- âœ… Reiniciar LLM sin afectar app

**Desventajas:**
- âš ï¸ ConfiguraciÃ³n mÃ¡s compleja
- âš ï¸ Latencia de red entre contenedores (mÃ­nima)

### Arquitectura 3: HÃ­brida (Local + Cloud Fallback)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Docker Container (Local)  â”‚
â”‚                             â”‚
â”‚   Sistema ValidaciÃ³n v5     â”‚
â”‚           â†“                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Router Inteligente â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚        â†“           â†“        â”‚
â”‚   Local LLM    OpenAI API   â”‚
â”‚   (Phi-3.5)    (Fallback)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LÃ³gica:
- Tareas simples â†’ LLM local
- Tareas complejas â†’ GPT-4o-mini API
```

**Ventajas:**
- âœ… Mejor calidad global
- âœ… Privacidad para casos simples
- âœ… Costo reducido (70-80% de llamadas locales)

**Desventajas:**
- âš ï¸ AÃºn requiere conectividad
- âš ï¸ Complejidad adicional en enrutamiento

---

## 5. ESTIMACIÃ“N DE RENDIMIENTO Y CALIDAD

### 5.1 Comparativa de PrecisiÃ³n Estimada

| ConfiguraciÃ³n | PrecisiÃ³n Criterio 1 | PrecisiÃ³n Criterio 2 | PrecisiÃ³n Criterio 3 | PrecisiÃ³n Promedio | PÃ©rdida vs Actual |
|---------------|---------------------|---------------------|---------------------|--------------------|-------------------|
| **Actual (GPT-4o-mini API)** | 85% | 87% | 86% | **86%** | - |
| **Phi-3.5 Mini (local)** | 72% | 75% 73% | **73%** | -13% |
| **Qwen 2.5 3B (local)** | 70% | 73% | 71% | **71%** | -15% |
| **Llama 3.2 3B (local)** | 68% | 71% | 69% | **69%** | -17% |
| **HÃ­brida (80% local + 20% API)** | 80% | 82% | 81% | **81%** | -5% |

### 5.2 Comparativa de Velocidad

**Actual (GPT-4o-mini API):**
- Latencia por llamada: ~2-3 segundos
- 26 llamadas/puesto: ~60-80 segundos
- 25 puestos: ~15 minutos

**Con Phi-3.5 Mini Local:**
- Latencia por llamada: ~5-8 segundos (GPU)
- 26 llamadas/puesto: ~130-200 segundos
- 25 puestos: ~40-60 minutos

**Trade-off:** ğŸŒ 3-4x mÃ¡s lento pero completamente privado

### 5.3 AnÃ¡lisis de Costos

| Concepto | Actual (API) | Docker Local | Ahorro |
|----------|-------------|--------------|--------|
| **Costo por puesto** | $0.35 MXN | $0.00 MXN | 100% |
| **100 puestos/mes** | $35 MXN/mes | $0 MXN/mes | $35 MXN |
| **1,000 puestos/aÃ±o** | $350 MXN/aÃ±o | $0 MXN/aÃ±o | $350 MXN |
| **Costo de electricidad** | $0 | ~$2-5 MXN/mes | N/A |

**Punto de equilibrio:** Inmediato (si ya tienes el hardware)

---

## 6. ANÃLISIS DE RIESGOS

### 6.1 Riesgo: DegradaciÃ³n de Calidad (ALTO)

**Probabilidad:** ALTA
**Impacto:** ALTO

**DescripciÃ³n:** Modelos 3B tendrÃ¡n menor precisiÃ³n que GPT-4o-mini (73% vs 86%)

**MitigaciÃ³n:**
- OpciÃ³n 1: Arquitectura hÃ­brida (usar API para casos complejos)
- OpciÃ³n 2: Fine-tuning del modelo local con datos APF
- OpciÃ³n 3: Prompts mÃ¡s elaborados y especÃ­ficos
- OpciÃ³n 4: Aceptar la pÃ©rdida si la privacidad es crÃ­tica

### 6.2 Riesgo: VRAM Insuficiente (MEDIO)

**Probabilidad:** MEDIA
**Impacto:** ALTO

**DescripciÃ³n:** 6GB puede ser justo para Phi-3.5 + embeddings + overhead

**MitigaciÃ³n:**
- Usar cuantizaciÃ³n Q4_K_M (reduce VRAM ~40%)
- Descargar embeddings a CPU
- Limitar batch size a 1
- Monitorear uso de VRAM continuamente

### 6.3 Riesgo: Rendimiento Lento (MEDIO)

**Probabilidad:** ALTA
**Impacto:** MEDIO

**DescripciÃ³n:** 3-4x mÃ¡s lento que API (60s â†’ 180s por puesto)

**MitigaciÃ³n:**
- Procesamiento en batch durante la noche
- CachÃ© agresivo de resultados
- OptimizaciÃ³n de prompts (reducir tokens)
- Considerar GPU mÃ¡s potente en el futuro

### 6.4 Riesgo: Complejidad Operativa (MEDIO)

**Probabilidad:** MEDIA
**Impacto:** MEDIO

**DescripciÃ³n:** Docker + GPU + Ollama es mÃ¡s complejo que solo API

**MitigaciÃ³n:**
- DocumentaciÃ³n exhaustiva
- Scripts de setup automatizados
- Monitoreo y logs detallados
- Fallback a API si falla local

---

## 7. RECOMENDACIONES

### 7.1 RecomendaciÃ³n Principal: ENFOQUE HÃBRIDO

**Propuesta:** Implementar arquitectura hÃ­brida con enrutamiento inteligente

```python
def seleccionar_proveedor(tipo_tarea, complejidad, nivel_puesto):
    # Casos simples â†’ LLM Local (80% de casos)
    if complejidad == "baja" and tipo_tarea in ["AdvancedQuality", "Criterio3"]:
        return "phi-3.5-local"

    # Casos complejos â†’ GPT-4o-mini API (20% de casos)
    if nivel_puesto in ["G11", "H21"] or complejidad == "alta":
        return "gpt-4o-mini-api"

    # Criterio 2 (contextual) â†’ Siempre API (mayor precisiÃ³n)
    if tipo_tarea == "Criterio2":
        return "gpt-4o-mini-api"

    # Default
    return "phi-3.5-local"
```

**Resultado esperado:**
- **Costo:** $0.07 MXN/puesto (80% reducciÃ³n vs actual)
- **PrecisiÃ³n:** ~81% (vs 86% actual, solo -5%)
- **Velocidad:** Similar a actual (casos simples rÃ¡pidos localmente)
- **Privacidad:** 80% de procesamiento offline

### 7.2 Si Requieres 100% Local (Sin API)

**OpciÃ³n Recomendada:** Phi-3.5 Mini + Optimizaciones

**Pasos clave:**
1. âœ… Usar Phi-3.5 Mini 3.8B cuantizado Q4_K_M
2. âœ… Prompts muy especÃ­ficos y detallados
3. âœ… Fine-tuning con 50-100 ejemplos reales de APF
4. âœ… Sistema de validaciÃ³n humana para casos edge
5. âœ… Aceptar 73% de precisiÃ³n (vs 86%)

**Trade-offs aceptables:**
- âŒ -13% de precisiÃ³n
- âœ… $350 MXN/aÃ±o de ahorro (1000 puestos)
- âœ… 100% privacidad y datos locales
- âœ… Sin dependencia de internet

### 7.3 Si Tienes Flexibilidad de Hardware

**InversiÃ³n recomendada:** GPU con 12-16GB VRAM (~$300-500 USD)

**Beneficios:**
- âœ… Usar Llama 3.1 8B (mucho mejor que 3B)
- âœ… PrecisiÃ³n: ~82-84% (muy cerca de GPT-4o-mini)
- âœ… Velocidad similar o superior
- âœ… ROI en 1-2 aÃ±os con volumen medio

---

## 8. PLAN DE IMPLEMENTACIÃ“N PROPUESTO

### Fase 1: Proof of Concept (1 semana)

**Objetivos:**
- Dockerizar con Ollama + Phi-3.5 Mini
- Probar con 10 puestos reales
- Medir precisiÃ³n, velocidad, uso de recursos

**Entregables:**
- `Dockerfile` + `docker-compose.yml`
- README de instalaciÃ³n
- Reporte de benchmarks

### Fase 2: OptimizaciÃ³n (1-2 semanas)

**Objetivos:**
- Implementar enrutamiento hÃ­brido
- Optimizar prompts para Phi-3.5
- Agregar cachÃ© agresivo
- Validar con 100 puestos

**Entregables:**
- Sistema hÃ­brido funcional
- Comparativa de calidad vs API
- Dashboard de monitoreo

### Fase 3: ProducciÃ³n (1 semana)

**Objetivos:**
- DocumentaciÃ³n completa
- Scripts de deployment
- Monitoreo y alertas
- Plan de rollback a API

**Entregables:**
- Sistema listo para uso
- GuÃ­a de operaciÃ³n
- Plan de mantenimiento

---

## 9. ALTERNATIVAS DESCARTADAS

### âŒ Alternativa 1: Usar Modelos 7B+ con CPU Only

**Por quÃ© no:** Extremadamente lento (10-20x mÃ¡s lento), inviable para 26 llamadas/puesto

### âŒ Alternativa 2: Usar APIs Locales Open Source (LocalAI, etc.)

**Por quÃ© no:** Mismo problema de VRAM, sin ventajas adicionales vs Ollama

### âŒ Alternativa 3: Usar Modelos Especializados Legales (LLaMA-Legal, etc.)

**Por quÃ© no:** Suelen ser 7B+, no caben en 6GB VRAM

---

## 10. PREGUNTAS PARA DISCUSIÃ“N

Antes de proceder, necesito que respondas estas preguntas clave:

### Pregunta 1: Prioridad Principal
Â¿CuÃ¡l es tu prioridad #1?
- **A) Privacidad total** (0% de datos enviados a internet, acepto -13% de precisiÃ³n)
- **B) MÃ¡xima calidad** (quiero mantener ~80%+ de precisiÃ³n, acepto usar API hÃ­brido)
- **C) MÃ­nimo costo** (quiero $0 de costos operativos, acepto trade-offs)

### Pregunta 2: Contexto de Uso
Â¿CÃ³mo planeas usar el sistema?
- **A) Gobierno/InstituciÃ³n pÃºblica** (datos sensibles, requiere privacidad)
- **B) ConsultorÃ­a privada** (flexible, calidad es clave)
- **C) Uso personal/acadÃ©mico** (experimentaciÃ³n, aprendizaje)

### Pregunta 3: Volumen Esperado
Â¿CuÃ¡ntos puestos analizarÃ¡s tÃ­picamente?
- **A) <100 puestos/mes** (bajo volumen)
- **B) 100-500 puestos/mes** (volumen medio)
- **C) >500 puestos/mes** (alto volumen)

### Pregunta 4: Tolerancia a Lentitud
Â¿Puedes aceptar que el anÃ¡lisis sea 3-4x mÃ¡s lento?
- **A) SÃ­, no hay problema** (puedo esperar 40-60 min para 25 puestos)
- **B) Prefiero velocidad** (necesito resultados en <20 minutos)

### Pregunta 5: Hardware Future
Â¿Tienes planes de mejorar el hardware?
- **A) SÃ­, podrÃ­a invertir en GPU mejor** (~$300-500 USD)
- **B) No, debo trabajar con 6GB VRAM**

---

## 11. CONCLUSIÃ“N PRELIMINAR

**Veredicto tÃ©cnico:** âœ… **ES VIABLE** dockerizar con LLM local en 6GB VRAM

**Pero con advertencias importantes:**
- âš ï¸ PÃ©rdida de 10-15% de precisiÃ³n (86% â†’ 71-76%)
- âš ï¸ 3-4x mÃ¡s lento que API actual
- âš ï¸ Requiere fine-tuning y optimizaciÃ³n de prompts
- âš ï¸ VRAM estÃ¡ al lÃ­mite (poco margen de error)

**RecomendaciÃ³n final:** ğŸ† **ENFOQUE HÃBRIDO** (80% local + 20% API)
- Mejor balance entre costo, calidad y privacidad
- Reduce precisiÃ³n solo 5% (86% â†’ 81%)
- Ahorra 80% de costos ($0.35 â†’ $0.07 MXN/puesto)
- Mantiene velocidad razonable

---

**Siguiente paso:** Responde las 5 preguntas y definiremos la arquitectura exacta a implementar.
